{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsKnnrZ9zDMo",
        "outputId": "ee8de47e-213b-4574-f2fe-fec02197e772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'capstone_l2r_ess'...\n",
            "remote: Enumerating objects: 778, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 778 (delta 54), reused 87 (delta 45), pack-reused 662 (from 1)\u001b[K\n",
            "Receiving objects: 100% (778/778), 27.14 MiB | 21.55 MiB/s, done.\n",
            "Resolving deltas: 100% (270/270), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/DngBack/capstone_l2r_ess.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWPeeOv1zN-f",
        "outputId": "698d52e8-e96a-4e4f-c73d-1014a1718530"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/capstone_l2r_ess\n"
          ]
        }
      ],
      "source": [
        "!cd /content/capstone_l2r_ess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEiUasKPzTe8",
        "outputId": "bd74df16-0855-456b-8982-f321b6bce080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Collecting hydra-core (from -r requirements.txt (line 8))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (2.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (13.9.4)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (0.8.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (8.4.2)\n",
            "Collecting pytest-mock (from -r requirements.txt (line 15))\n",
            "  Downloading pytest_mock-3.15.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core->-r requirements.txt (line 8)) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->-r requirements.txt (line 9)) (6.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 10)) (3.2.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements.txt (line 12)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements.txt (line 12)) (2.19.2)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 14)) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->-r requirements.txt (line 14)) (1.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 2)) (3.0.3)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_mock-3.15.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pytest-mock, hydra-core\n",
            "Successfully installed hydra-core-1.3.2 pytest-mock-3.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AwRD_x070LUi"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SueApVj1HAN",
        "outputId": "31c9accf-ae42-47bc-d8c2-4436cd39ec23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/capstone_l2r_ess/data\n"
          ]
        }
      ],
      "source": [
        "!cd data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2IwFyhl1U1x",
        "outputId": "a427d815-8f32-4847-aad1-312eae1ac755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cifar-100-python/\n",
            "cifar-100-python/file.txt~\n",
            "cifar-100-python/train\n",
            "cifar-100-python/test\n",
            "cifar-100-python/meta\n"
          ]
        }
      ],
      "source": [
        "!tar -xvzf /content/capstone_l2r_ess/data/cifar-100-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elq8ZAII2NjC",
        "outputId": "0ae10115-bf3c-48eb-f2a0-c6b9b5e0e62c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/capstone_l2r_ess\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNKQjgZp1X76",
        "outputId": "ae5a75a8-a848-4d9f-cf97-028963f0761f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CREATING CIFAR-100-LT WITH BALANCED TEST SPLITS\n",
            "================================================================================\n",
            "\n",
            "Configuration:\n",
            "  Imbalance Factor: 100\n",
            "  Output Directory: data/cifar100_lt_if100_splits_fixed\n",
            "  Random Seed: 42\n",
            "  Train Split: 90% Expert / 10% Gating\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Loading CIFAR-100 datasets...\n",
            "================================================================================\n",
            "  - Train: 50,000 samples\n",
            "  - Test:  10,000 samples\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Creating long-tail training set...\n",
            "================================================================================\n",
            "Creating CIFAR-100-LT training set (IF=100)...\n",
            "  Total samples: 10,847\n",
            "  Head class (0): 500 samples\n",
            "  Tail class (99): 5 samples\n",
            "  Imbalance Factor: 100.0\n",
            "\n",
            "================================================================================\n",
            "STEP 3: Splitting train set (Expert vs Gating)...\n",
            "================================================================================\n",
            "\n",
            "Splitting training set (90% expert / 10% gating)...\n",
            "\n",
            "  SUCCESS: Expert split:\n",
            "    Total: 9,719 samples\n",
            "    Head class (0): 450 samples\n",
            "    Tail class (99): 4 samples\n",
            "    Imbalance Factor: 112.5\n",
            "\n",
            "  SUCCESS: Gating split:\n",
            "    Total: 1,128 samples\n",
            "    Head class (0): 50 samples\n",
            "    Tail class (99): 1 samples\n",
            "    Imbalance Factor: 50.0\n",
            "\n",
            "  SUCCESS: No overlap between expert and gating splits\n",
            "\n",
            "  SUCCESS: Imbalance ratio preserved:\n",
            "    Original: 100.0\n",
            "    Expert:   112.5\n",
            "    Gating:   50.0\n",
            "\n",
            "================================================================================\n",
            "STEP 4: Splitting test set (8:1:1)...\n",
            "================================================================================\n",
            "\n",
            "Splitting CIFAR-100 test set according to correct methodology...\n",
            "  Original: 100 samples per class (10,000 total)\n",
            "  Step 1: Split into Sval (20%) and Stest (80%)\n",
            "    - Sval:  20 per class (2,000 total)\n",
            "    - Stest: 80 per class (8,000 total)\n",
            "  Step 2: Split Sval into Val and TuneV (50% each)\n",
            "    - Val:   10 per class (1,000 total) = 1/2 of Sval\n",
            "    - TuneV: 10 per class (1,000 total) = 1/2 of Sval\n",
            "  Final result:\n",
            "    - Test:  80 per class (8,000 total)\n",
            "    - Val:   10 per class (1,000 total)\n",
            "    - TuneV: 10 per class (1,000 total)\n",
            "\n",
            "  SUCCESS: Splits created successfully:\n",
            "    Test (from Stest):  8,000 samples (8000 unique)\n",
            "    Val (1/2 of Sval):  1,000 samples (1000 unique)\n",
            "    TuneV (1/2 of Sval): 1,000 samples (1000 unique)\n",
            "    Total: 10,000 / 10,000\n",
            "  SUCCESS: No data leakage - all splits are disjoint\n",
            "  SUCCESS: All splits are perfectly balanced\n",
            "  SUCCESS: TuneV equals Val (both are 1/2 of Sval)\n",
            "\n",
            "================================================================================\n",
            "STEP 5: Computing class weights...\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "CLASS WEIGHTS (for metric reweighting)\n",
            "============================================================\n",
            "Total training samples: 10,847\n",
            "\n",
            "Weight distribution:\n",
            "  Head class (0):   500 samples -> weight = 0.046096\n",
            "  Class 25:         156 samples -> weight = 0.014382\n",
            "  Class 50:          48 samples -> weight = 0.004425\n",
            "  Class 75:          15 samples -> weight = 0.001383\n",
            "  Tail class (99):    5 samples -> weight = 0.000461\n",
            "\n",
            "Weight ratio (head/tail): 100.0x\n",
            "============================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 6: Analyzing distributions...\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "TRAIN (ORIGINAL) DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 10,847\n",
            "Samples per class:\n",
            "  Head class (0):  500\n",
            "  Class 25:        156\n",
            "  Class 50:        48\n",
            "  Class 75:        15\n",
            "  Tail class (99): 5\n",
            "Imbalance factor: 100.0\n",
            "\n",
            "============================================================\n",
            "TRAIN EXPERT DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 9,719\n",
            "Samples per class:\n",
            "  Head class (0):  450\n",
            "  Class 25:        140\n",
            "  Class 50:        43\n",
            "  Class 75:        13\n",
            "  Tail class (99): 4\n",
            "Imbalance factor: 112.5\n",
            "\n",
            "============================================================\n",
            "TRAIN GATING DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 1,128\n",
            "Samples per class:\n",
            "  Head class (0):  50\n",
            "  Class 25:        16\n",
            "  Class 50:        5\n",
            "  Class 75:        2\n",
            "  Tail class (99): 1\n",
            "Imbalance factor: 50.0\n",
            "\n",
            "============================================================\n",
            "TEST DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 8,000\n",
            "Samples per class:\n",
            "  Head class (0):  80\n",
            "  Class 25:        80\n",
            "  Class 50:        80\n",
            "  Class 75:        80\n",
            "  Tail class (99): 80\n",
            "Imbalance factor: 1.0\n",
            "SUCCESS: Perfectly balanced: 80 samples per class\n",
            "\n",
            "============================================================\n",
            "VAL DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 1,000\n",
            "Samples per class:\n",
            "  Head class (0):  10\n",
            "  Class 25:        10\n",
            "  Class 50:        10\n",
            "  Class 75:        10\n",
            "  Tail class (99): 10\n",
            "Imbalance factor: 1.0\n",
            "SUCCESS: Perfectly balanced: 10 samples per class\n",
            "\n",
            "============================================================\n",
            "TUNEV DISTRIBUTION\n",
            "============================================================\n",
            "Total samples: 1,000\n",
            "Samples per class:\n",
            "  Head class (0):  10\n",
            "  Class 25:        10\n",
            "  Class 50:        10\n",
            "  Class 75:        10\n",
            "  Tail class (99): 10\n",
            "Imbalance factor: 1.0\n",
            "SUCCESS: Perfectly balanced: 10 samples per class\n",
            "\n",
            "================================================================================\n",
            "STEP 7: Saving splits...\n",
            "================================================================================\n",
            "\n",
            "Saving splits to: data/cifar100_lt_if100_splits_fixed\n",
            "  - train_indices.json (10,847 items)\n",
            "  - test_indices.json (8,000 items)\n",
            "  - val_indices.json (1,000 items)\n",
            "  - tunev_indices.json (1,000 items)\n",
            "  - class_weights.json (100 items)\n",
            "  - train_class_counts.json (100 items)\n",
            "  - expert_indices.json (9,719 items)\n",
            "  - gating_indices.json (1,128 items)\n",
            "  - expert_class_counts.json (100 items)\n",
            "  - gating_class_counts.json (100 items)\n",
            "\n",
            "================================================================================\n",
            "STEP 8: Creating dataset objects...\n",
            "================================================================================\n",
            "  - train: 10,847 samples\n",
            "  - test: 8,000 samples\n",
            "  - val: 1,000 samples\n",
            "  - tunev: 1,000 samples\n",
            "  - expert: 9,719 samples\n",
            "  - gating: 1,128 samples\n",
            "\n",
            "================================================================================\n",
            "SUCCESS: DATASET CREATION COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "  Train (Original): 10,847 samples (long-tail, IF=100)\n",
            "  Train (Expert):   9,719 samples (long-tail, IF=100)\n",
            "  Train (Gating):   1,128 samples (long-tail, IF=100)\n",
            "  Test:   8,000 samples (balanced)\n",
            "  Val:    1,000 samples (balanced)\n",
            "  TuneV:  1,000 samples (balanced)\n",
            "\n",
            "Files saved:\n",
            "  data/cifar100_lt_if100_splits_fixed/train_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/expert_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/gating_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/expert_class_counts.json\n",
            "  data/cifar100_lt_if100_splits_fixed/gating_class_counts.json\n",
            "  data/cifar100_lt_if100_splits_fixed/test_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/val_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/tunev_indices.json\n",
            "  data/cifar100_lt_if100_splits_fixed/class_weights.json  <-- USE for metrics!\n",
            "  data/cifar100_lt_if100_splits_fixed/train_class_counts.json\n",
            "\n",
            "IMPORTANT:\n",
            "  - Test/Val/TuneV are BALANCED (no duplication)\n",
            "  - Expert/Gating maintain same imbalance ratio\n",
            "  - Use class_weights.json for reweighted metrics\n",
            "  - No data leakage between splits\n",
            "\n",
            "============================================================\n",
            "READY FOR TRAINING!\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "  1. Train experts with: python train_experts.py\n",
            "  2. Train gating with: python train_gating.py\n",
            "  3. Evaluate with reweighted metrics\n"
          ]
        }
      ],
      "source": [
        "!python src/data/balanced_test_splits.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01FQXpTw1YAG",
        "outputId": "8a11b28f-cad6-476f-9471-293afaf84b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "LOGGING TO FILE: logs/experts_cifar.log\n",
            "STARTED AT: 2025-12-04 10:12:48\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "AR-GSE EXPERT TRAINING\n",
            "============================================================\n",
            "ðŸ“Š Training Mode: Using FULL train set (default)\n",
            "   - Trains on full train set (100% of train)\n",
            "   - Validates on balanced val set\n",
            "   - Uses reweighted metrics for validation\n",
            "\n",
            "âœ“ Using dataset: cifar100_lt_if100\n",
            "  Classes: 100\n",
            "  Backbone: cifar_resnet32\n",
            "  Batch size: 128\n",
            "ðŸš€ AR-GSE Expert Training Pipeline\n",
            "Device: cpu\n",
            "Dataset: cifar100_lt_if100\n",
            "Experts to train: ['ce', 'logitadjust', 'balsoftmax']\n",
            "\n",
            "========================================\n",
            "ðŸŽ¯ Training Expert: CE\n",
            "========================================\n",
            "\n",
            "============================================================\n",
            "[EXPERT] TRAINING EXPERT: CE_BASELINE\n",
            "[EXPERT] Loss Type: CE\n",
            "[EXPERT] Splits Dir: ./data/cifar100_lt_if100_splits_fixed\n",
            "============================================================\n",
            "Loading CIFAR-100-LT datasets...\n",
            "  Using FULL train split for training\n",
            "Setting up CIFAR-100-LT datasets...\n",
            "  train: 10,847 samples\n",
            "  val: 1,000 samples\n",
            "  test: 8,000 samples\n",
            "  tunev: 1,000 samples\n",
            "Datasets setup complete!\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  Train loader: 84 batches (10,847 samples)\n",
            "  Val loader: 8 batches (1,000 samples)\n",
            "[SUCCESS] Loss Function: CrossEntropyLoss\n",
            "[INFO] Model Architecture:\n",
            "Expert Model Summary:\n",
            "  Backbone: 466,256 parameters\n",
            "  Classifier: 6,500 parameters\n",
            "  Total: 472,756 parameters\n",
            "  Temperature: 1.0000\n",
            "[INFO] Using manual LR scheduling (warmup=15 epochs, decay 0.1 at epochs [96, 192, 224])\n",
            "Epoch 1/256:   0% 0/84 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1/256:   0% 0/84 [00:03<?, ?it/s]\n",
            "\n",
            "\n",
            "âš ï¸  Training interrupted by user\n"
          ]
        }
      ],
      "source": [
        "!python train_experts.py --dataset cifar100_lt_if100 --log-file logs/experts_cifar.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bKilkkk1YCP",
        "outputId": "6e49da11-9fed-44aa-bf0a-5ac0daf1fb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Using dataset: cifar100_lt_if100\n",
            "  Classes: 100\n",
            "  Experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "======================================================================\n",
            "TRAINING GATING NETWORK FOR MAP\n",
            "======================================================================\n",
            "Loading expert logits and labels...\n",
            "  Loading gating split (10% of train with same long-tail)...\n",
            "    Train: 1,128 samples, 3 experts, 100 classes\n",
            "    Train logits shape: torch.Size([1128, 3, 100])\n",
            "    Train labels shape: torch.Size([1128])\n",
            "  Loading val split...\n",
            "    Val: 1,000 samples\n",
            "    Val logits shape: torch.Size([1000, 3, 100])\n",
            "    Val labels shape: torch.Size([1000])\n",
            "[OK] Loaded class weights (range: [0.0005, 0.0461])\n",
            "\n",
            "Creating GatingNetwork:\n",
            "   Experts: 3\n",
            "   Classes: 100\n",
            "   Routing: dense\n",
            "   Hidden: [256, 128]\n",
            "   Total params: 114,691\n",
            "   Feature dim: 314\n",
            "\n",
            "Loss Configuration:\n",
            "   Mixture NLL: âœ“\n",
            "   Load-balancing: âœ— (disabled for dense routing)\n",
            "   Entropy reg: âœ“\n",
            "   Responsibility loss: âœ“\n",
            "   Prior regularizer: âœ“\n",
            "   Î»_H: 0.01\n",
            "   Î»_Resp: 0.1\n",
            "   Î»_Prior: 0.05\n",
            "\n",
            "Estimating group priors from training data...\n",
            "   Group priors shape: torch.Size([2, 3])\n",
            "   Priors for each group:\n",
            "tensor([[0.4213, 0.2187, 0.3601],\n",
            "        [0.4293, 0.1413, 0.4293]])\n",
            "\n",
            "Starting training for 100 epochs...\n",
            "   Batch size: 128\n",
            "   Learning rate: 0.001\n",
            "   Warmup epochs: 5\n",
            "   Router temp: 2.0 â†’ 0.7\n",
            "\n",
            "Epoch   1/100:\n",
            "  Train Loss: 0.0123 (NLL=0.0025)\n",
            " LR=0.000200\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch   2/100:\n",
            "  Train Loss: 0.0087 (NLL=0.0024)\n",
            " LR=0.000400\n",
            "  Mixture Acc: 0.9991, Effective Experts: 2.96\n",
            "\n",
            "Epoch   3/100:\n",
            "  Train Loss: 0.0077 (NLL=0.0023)\n",
            " LR=0.000600\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.97\n",
            "\n",
            "Epoch   4/100:\n",
            "  Train Loss: 0.0067 (NLL=0.0022)\n",
            " LR=0.000800\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch   5/100:\n",
            "  Train Loss: 0.0060 (NLL=0.0024)\n",
            " LR=0.001000\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9306 (NLL=1.9304)\n",
            "  Val Acc: Overall=0.4780, Head=0.6360, Tail=0.3200, Balanced=0.4780\n",
            "  Saved best model (balanced_acc=0.4780)\n",
            "\n",
            "Epoch   6/100:\n",
            "  Train Loss: 0.0056 (NLL=0.0023)\n",
            " LR=0.001000\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch   7/100:\n",
            "  Train Loss: 0.0054 (NLL=0.0023)\n",
            " LR=0.000999\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch   8/100:\n",
            "  Train Loss: 0.0053 (NLL=0.0023)\n",
            " LR=0.000998\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch   9/100:\n",
            "  Train Loss: 0.0051 (NLL=0.0023)\n",
            " LR=0.000996\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  10/100:\n",
            "  Train Loss: 0.0049 (NLL=0.0023)\n",
            " LR=0.000994\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9426 (NLL=1.9422)\n",
            "  Val Acc: Overall=0.4780, Head=0.6320, Tail=0.3240, Balanced=0.4780\n",
            "  Saved best model (balanced_acc=0.4780)\n",
            "\n",
            "Epoch  11/100:\n",
            "  Train Loss: 0.0048 (NLL=0.0022)\n",
            " LR=0.000991\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  12/100:\n",
            "  Train Loss: 0.0048 (NLL=0.0023)\n",
            " LR=0.000988\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  13/100:\n",
            "  Train Loss: 0.0047 (NLL=0.0023)\n",
            " LR=0.000984\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  14/100:\n",
            "  Train Loss: 0.0046 (NLL=0.0023)\n",
            " LR=0.000980\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  15/100:\n",
            "  Train Loss: 0.0046 (NLL=0.0022)\n",
            " LR=0.000976\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9407 (NLL=1.9403)\n",
            "  Val Acc: Overall=0.4790, Head=0.6340, Tail=0.3240, Balanced=0.4790\n",
            "  Saved best model (balanced_acc=0.4790)\n",
            "\n",
            "Epoch  16/100:\n",
            "  Train Loss: 0.0045 (NLL=0.0022)\n",
            " LR=0.000970\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  17/100:\n",
            "  Train Loss: 0.0045 (NLL=0.0023)\n",
            " LR=0.000965\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  18/100:\n",
            "  Train Loss: 0.0045 (NLL=0.0023)\n",
            " LR=0.000959\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  19/100:\n",
            "  Train Loss: 0.0044 (NLL=0.0022)\n",
            " LR=0.000952\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  20/100:\n",
            "  Train Loss: 0.0044 (NLL=0.0022)\n",
            " LR=0.000946\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9337 (NLL=1.9334)\n",
            "  Val Acc: Overall=0.4830, Head=0.6400, Tail=0.3260, Balanced=0.4830\n",
            "  Saved best model (balanced_acc=0.4830)\n",
            "\n",
            "Epoch  21/100:\n",
            "  Train Loss: 0.0044 (NLL=0.0023)\n",
            " LR=0.000938\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  22/100:\n",
            "  Train Loss: 0.0044 (NLL=0.0022)\n",
            " LR=0.000930\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  23/100:\n",
            "  Train Loss: 0.0044 (NLL=0.0022)\n",
            " LR=0.000922\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  24/100:\n",
            "  Train Loss: 0.0043 (NLL=0.0022)\n",
            " LR=0.000914\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  25/100:\n",
            "  Train Loss: 0.0043 (NLL=0.0023)\n",
            " LR=0.000905\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9411 (NLL=1.9408)\n",
            "  Val Acc: Overall=0.4780, Head=0.6340, Tail=0.3220, Balanced=0.4780\n",
            "\n",
            "Epoch  26/100:\n",
            "  Train Loss: 0.0043 (NLL=0.0022)\n",
            " LR=0.000895\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  27/100:\n",
            "  Train Loss: 0.0043 (NLL=0.0022)\n",
            " LR=0.000885\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  28/100:\n",
            "  Train Loss: 0.0043 (NLL=0.0022)\n",
            " LR=0.000875\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  29/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000864\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  30/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000854\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9447 (NLL=1.9443)\n",
            "  Val Acc: Overall=0.4780, Head=0.6360, Tail=0.3200, Balanced=0.4780\n",
            "\n",
            "Epoch  31/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000842\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  32/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000831\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  33/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000819\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  34/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000806\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  35/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000794\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9402 (NLL=1.9398)\n",
            "  Val Acc: Overall=0.4790, Head=0.6340, Tail=0.3240, Balanced=0.4790\n",
            "\n",
            "Epoch  36/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000781\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  37/100:\n",
            "  Train Loss: 0.0042 (NLL=0.0022)\n",
            " LR=0.000768\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  38/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000755\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  39/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000741\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  40/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000727\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9452 (NLL=1.9448)\n",
            "  Val Acc: Overall=0.4780, Head=0.6360, Tail=0.3200, Balanced=0.4780\n",
            "\n",
            "Epoch  41/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000713\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  42/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000699\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  43/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000684\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  44/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000669\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  45/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000655\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9448 (NLL=1.9444)\n",
            "  Val Acc: Overall=0.4790, Head=0.6360, Tail=0.3220, Balanced=0.4790\n",
            "\n",
            "Epoch  46/100:\n",
            "  Train Loss: 0.0041 (NLL=0.0022)\n",
            " LR=0.000639\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  47/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000624\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  48/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000609\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  49/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000594\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  50/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000578\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "  Val Loss: 1.9461 (NLL=1.9457)\n",
            "  Val Acc: Overall=0.4790, Head=0.6360, Tail=0.3220, Balanced=0.4790\n",
            "\n",
            "Epoch  51/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000563\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  52/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000547\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  53/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000531\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  54/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000516\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.96\n",
            "\n",
            "Epoch  55/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0021)\n",
            " LR=0.000500\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9487 (NLL=1.9483)\n",
            "  Val Acc: Overall=0.4790, Head=0.6360, Tail=0.3220, Balanced=0.4790\n",
            "\n",
            "Epoch  56/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0021)\n",
            " LR=0.000484\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  57/100:\n",
            "  Train Loss: 0.0040 (NLL=0.0022)\n",
            " LR=0.000469\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  58/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0022)\n",
            " LR=0.000453\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  59/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0022)\n",
            " LR=0.000437\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  60/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000422\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9531 (NLL=1.9526)\n",
            "  Val Acc: Overall=0.4790, Head=0.6360, Tail=0.3220, Balanced=0.4790\n",
            "\n",
            "Epoch  61/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000406\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  62/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000391\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  63/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000376\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  64/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000361\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  65/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000345\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9559 (NLL=1.9555)\n",
            "  Val Acc: Overall=0.4770, Head=0.6340, Tail=0.3200, Balanced=0.4770\n",
            "\n",
            "Epoch  66/100:\n",
            "  Train Loss: 0.0039 (NLL=0.0021)\n",
            " LR=0.000331\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  67/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000316\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  68/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000301\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  69/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000287\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  70/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000273\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9590 (NLL=1.9585)\n",
            "  Val Acc: Overall=0.4770, Head=0.6340, Tail=0.3200, Balanced=0.4770\n",
            "\n",
            "Epoch  71/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000259\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  72/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000245\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  73/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000232\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  74/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000219\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  75/100:\n",
            "  Train Loss: 0.0038 (NLL=0.0021)\n",
            " LR=0.000206\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9634 (NLL=1.9629)\n",
            "  Val Acc: Overall=0.4750, Head=0.6320, Tail=0.3180, Balanced=0.4750\n",
            "\n",
            "Epoch  76/100:\n",
            "  Train Loss: 0.0037 (NLL=0.0021)\n",
            " LR=0.000194\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  77/100:\n",
            "  Train Loss: 0.0037 (NLL=0.0021)\n",
            " LR=0.000181\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  78/100:\n",
            "  Train Loss: 0.0037 (NLL=0.0021)\n",
            " LR=0.000169\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "\n",
            "Epoch  79/100:\n",
            "  Train Loss: 0.0037 (NLL=0.0021)\n",
            " LR=0.000158\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  80/100:\n",
            "  Train Loss: 0.0037 (NLL=0.0021)\n",
            " LR=0.000146\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.95\n",
            "  Val Loss: 1.9696 (NLL=1.9690)\n",
            "  Val Acc: Overall=0.4740, Head=0.6320, Tail=0.3160, Balanced=0.4740\n",
            "\n",
            "Epoch  81/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0020)\n",
            " LR=0.000136\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  82/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0021)\n",
            " LR=0.000125\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  83/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0020)\n",
            " LR=0.000115\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  84/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0020)\n",
            " LR=0.000105\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  85/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0020)\n",
            " LR=0.000095\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "  Val Loss: 1.9774 (NLL=1.9768)\n",
            "  Val Acc: Overall=0.4720, Head=0.6300, Tail=0.3140, Balanced=0.4720\n",
            "\n",
            "Epoch  86/100:\n",
            "  Train Loss: 0.0036 (NLL=0.0020)\n",
            " LR=0.000086\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  87/100:\n",
            "  Train Loss: 0.0035 (NLL=0.0020)\n",
            " LR=0.000078\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  88/100:\n",
            "  Train Loss: 0.0035 (NLL=0.0020)\n",
            " LR=0.000070\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  89/100:\n",
            "  Train Loss: 0.0035 (NLL=0.0020)\n",
            " LR=0.000062\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  90/100:\n",
            "  Train Loss: 0.0035 (NLL=0.0020)\n",
            " LR=0.000054\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "  Val Loss: 1.9841 (NLL=1.9834)\n",
            "  Val Acc: Overall=0.4730, Head=0.6320, Tail=0.3140, Balanced=0.4730\n",
            "\n",
            "Epoch  91/100:\n",
            "  Train Loss: 0.0035 (NLL=0.0020)\n",
            " LR=0.000048\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  92/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000041\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  93/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000035\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.94\n",
            "\n",
            "Epoch  94/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000030\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "\n",
            "Epoch  95/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000024\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "  Val Loss: 1.9894 (NLL=1.9887)\n",
            "  Val Acc: Overall=0.4720, Head=0.6320, Tail=0.3120, Balanced=0.4720\n",
            "\n",
            "Epoch  96/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000020\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "\n",
            "Epoch  97/100:\n",
            "  Train Loss: 0.0034 (NLL=0.0020)\n",
            " LR=0.000016\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "\n",
            "Epoch  98/100:\n",
            "  Train Loss: 0.0033 (NLL=0.0020)\n",
            " LR=0.000012\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "\n",
            "Epoch  99/100:\n",
            "  Train Loss: 0.0033 (NLL=0.0020)\n",
            " LR=0.000009\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "\n",
            "Epoch 100/100:\n",
            "  Train Loss: 0.0033 (NLL=0.0019)\n",
            " LR=0.000006\n",
            "  Mixture Acc: 1.0000, Effective Experts: 2.93\n",
            "  Val Loss: 1.9950 (NLL=1.9942)\n",
            "  Val Acc: Overall=0.4700, Head=0.6320, Tail=0.3080, Balanced=0.4700\n",
            "\n",
            "Training completed!\n",
            "   Best balanced acc: 0.4830\n",
            "   Best val loss: 1.9337\n",
            "   Checkpoints saved to: checkpoints/gating_map/cifar100_lt_if100\n",
            "\n",
            "ðŸŽ‰ Done!\n"
          ]
        }
      ],
      "source": [
        "!python -m src.train.train_gating_map --dataset cifar100_lt_if100 --routing dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDhAWp1J1YEl",
        "outputId": "feedcfbe-1fc8-459f-f7b3-7f57e09dd750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Using dataset: cifar100_lt_if100\n",
            "  Classes: 100\n",
            "  Experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "Loading gating network...\n",
            "Loading gating network for 3 experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "âœ“ Loaded gating network from: checkpoints/gating_map/cifar100_lt_if100/final_gating.pth\n",
            "âœ“ Gating network configured for 3 experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "Loading S1 (tunev) and S2 (val) for selection/evaluation...\n",
            "Loading logits for 3 experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "  âœ“ Loaded ce_baseline: torch.Size([1000, 100])\n",
            "  âœ“ Loaded logitadjust_baseline: torch.Size([1000, 100])\n",
            "  âœ“ Loaded balsoftmax_baseline: torch.Size([1000, 100])\n",
            "âœ“ Stacked expert logits: torch.Size([1000, 3, 100]) (should be [N, 3, 100])\n",
            "Loading logits for 3 experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "  âœ“ Loaded ce_baseline: torch.Size([1000, 100])\n",
            "  âœ“ Loaded logitadjust_baseline: torch.Size([1000, 100])\n",
            "  âœ“ Loaded balsoftmax_baseline: torch.Size([1000, 100])\n",
            "âœ“ Stacked expert logits: torch.Size([1000, 3, 100]) (should be [N, 3, 100])\n",
            "Computing mixture posteriors using gating network...\n",
            "Building class-to-group mapping (tail <= 20)...\n",
            "Groups: head=69, tail=31\n",
            "Loading class weights for importance weighting...\n",
            "Training distribution: head=0.046096, tail=0.000461\n",
            "Test distribution (balanced): 0.010000\n",
            "Importance weights: head=4.609569, tail=0.046096\n",
            "Weight ratio (head/tail): 100.0x\n",
            "Loading logits for 3 experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "  âœ“ Loaded ce_baseline: torch.Size([8000, 100])\n",
            "  âœ“ Loaded logitadjust_baseline: torch.Size([8000, 100])\n",
            "  âœ“ Loaded balsoftmax_baseline: torch.Size([8000, 100])\n",
            "âœ“ Stacked expert logits: torch.Size([8000, 3, 100]) (should be [N, 3, 100])\n",
            "Baseline Gating balanced error (TEST) = 0.5363\n",
            "Baseline Gating group errors = [0.31377703987553845, 0.7588075975642734]\n",
            "Baseline Gating overall accuracy (TEST) = 0.4601\n",
            "Creating plug-in model...\n",
            "\n",
            "=== Target 1/9: rejection=0.0 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.94000664 0.05999277]\n",
            "     mu= -2.0: alpha=[1.94000719 0.05999277]\n",
            "     mu= -1.0: alpha=[1.94000719 0.05999277]\n",
            "     mu=  0.0: alpha=[1.93999654 0.05992654]\n",
            "     mu=  1.0: alpha=[1.94000719 0.05999273]\n",
            "     mu=  2.0: alpha=[1.94000719 0.05999277]\n",
            "     mu=  3.0: alpha=[1.94000714 0.05999277]\n",
            "     mu=  5.0: alpha=[1.94000719 0.05999277]\n",
            "     mu=  6.0: alpha=[1.94000719 0.05999277]\n",
            "     mu=  8.0: alpha=[1.94000699 0.05999076]\n",
            "     mu= 11.0: alpha=[1.94000699 0.05999076]\n",
            "     mu= 15.0: alpha=[1.94000699 0.05999076]\n",
            "     mu= 20.0: alpha=[1.94000699 0.05999277]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu= -2.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu= -1.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  0.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  1.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  2.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  3.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  5.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  6.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu=  8.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu= 11.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu= 15.0: val_bal=0.4960 val_cov=1.000\n",
            "     mu= 20.0: val_bal=0.4960 val_cov=1.000\n",
            "   Step 3: Local refinement around best mu...\n",
            "   Final selection: mu=-5.00 val_bal=0.4960 val_cov=1.000\n",
            "   Best alpha: [1.94000664 0.05999277]\n",
            "   Alpha sum: 2.0000 (should be â‰ˆ 2.0000)\n",
            "   Alpha_hat (=alpha/K): [0.97000332 0.02999639]\n",
            "   Target=0.0  VAL: bal=0.4960 cov=1.000\n",
            "   Target=0.0  TEST: bal=0.5119 cov=1.000\n",
            "      VAL: worst=0.5462 | head=0.5462 | tail=0.4458 | tail-head=-0.1004\n",
            "      TEST: worst=0.5617 | head=0.5617 | tail=0.4622 | tail-head=-0.0995\n",
            "   alpha (group) learned = [1.94000664 0.05999277]\n",
            "   Baseline Gating acc (TEST) = 0.4601\n",
            "   Plugin balanced complement (1-bErr) ~ 0.4881\n",
            "\n",
            "=== Target 2/9: rejection=0.1 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.84201136 0.04881613]\n",
            "     mu= -2.0: alpha=[1.83992726 0.04898542]\n",
            "     mu= -1.0: alpha=[1.83754508 0.04933678]\n",
            "     mu=  0.0: alpha=[1.83514619 0.04935982]\n",
            "     mu=  1.0: alpha=[1.83358886 0.04950592]\n",
            "     mu=  2.0: alpha=[1.83014574 0.04955606]\n",
            "     mu=  3.0: alpha=[1.83142397 0.04945152]\n",
            "     mu=  5.0: alpha=[1.83276789 0.04949136]\n",
            "     mu=  6.0: alpha=[1.82878407 0.0498108 ]\n",
            "     mu=  8.0: alpha=[1.83155468 0.04953271]\n",
            "     mu= 11.0: alpha=[1.82095076 0.05072287]\n",
            "     mu= 15.0: alpha=[1.82227777 0.05059871]\n",
            "     mu= 20.0: alpha=[1.79461975 0.05113126]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.4887 val_cov=0.900\n",
            "     mu= -2.0: val_bal=0.4869 val_cov=0.900\n",
            "     mu= -1.0: val_bal=0.4837 val_cov=0.899\n",
            "     mu=  0.0: val_bal=0.4829 val_cov=0.900\n",
            "     mu=  1.0: val_bal=0.4821 val_cov=0.900\n",
            "     mu=  2.0: val_bal=0.4812 val_cov=0.900\n",
            "     mu=  3.0: val_bal=0.4806 val_cov=0.900\n",
            "     mu=  5.0: val_bal=0.4761 val_cov=0.900\n",
            "     mu=  6.0: val_bal=0.4704 val_cov=0.900\n",
            "     mu=  8.0: val_bal=0.4709 val_cov=0.900\n",
            "     mu= 11.0: val_bal=0.4656 val_cov=0.900\n",
            "     mu= 15.0: val_bal=0.4615 val_cov=0.900\n",
            "     mu= 20.0: val_bal=0.4631 val_cov=0.900\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 2: mu=16.00 val_bal=0.4608\n",
            "   Final selection: mu=16.00 val_bal=0.4608 val_cov=0.900\n",
            "   Best alpha: [1.81886604 0.05113142]\n",
            "   Alpha sum: 1.8700 (should be â‰ˆ 1.8000)\n",
            "   Alpha_hat (=alpha/K): [0.90943302 0.02556571]\n",
            "   Target=0.1  VAL: bal=0.4608 cov=0.900\n",
            "   Target=0.1  TEST: bal=0.4775 cov=0.900\n",
            "      VAL: worst=0.5364 | head=0.5364 | tail=0.3853 | tail-head=-0.1511\n",
            "      TEST: worst=0.5447 | head=0.5447 | tail=0.4103 | tail-head=-0.1344\n",
            "\n",
            "=== Target 3/9: rejection=0.2 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.74322662 0.03873588]\n",
            "     mu= -2.0: alpha=[1.73812266 0.03930914]\n",
            "     mu= -1.0: alpha=[1.72904158 0.03951213]\n",
            "     mu=  0.0: alpha=[1.71916989 0.03972441]\n",
            "     mu=  1.0: alpha=[1.71576009 0.04016229]\n",
            "     mu=  2.0: alpha=[1.71647096 0.0402884 ]\n",
            "     mu=  3.0: alpha=[1.71432198 0.04061044]\n",
            "     mu=  5.0: alpha=[1.71354933 0.04121161]\n",
            "     mu=  6.0: alpha=[1.70349363 0.04182446]\n",
            "     mu=  8.0: alpha=[1.70238149 0.04228996]\n",
            "     mu= 11.0: alpha=[1.69632941 0.04235965]\n",
            "     mu= 15.0: alpha=[1.64174942 0.04438246]\n",
            "     mu= 20.0: alpha=[1.64414919 0.04374803]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.4695 val_cov=0.800\n",
            "     mu= -2.0: val_bal=0.4577 val_cov=0.800\n",
            "     mu= -1.0: val_bal=0.4562 val_cov=0.800\n",
            "     mu=  0.0: val_bal=0.4509 val_cov=0.800\n",
            "     mu=  1.0: val_bal=0.4466 val_cov=0.800\n",
            "     mu=  2.0: val_bal=0.4456 val_cov=0.800\n",
            "     mu=  3.0: val_bal=0.4548 val_cov=0.800\n",
            "     mu=  5.0: val_bal=0.4484 val_cov=0.799\n",
            "     mu=  6.0: val_bal=0.4484 val_cov=0.799\n",
            "     mu=  8.0: val_bal=0.4406 val_cov=0.800\n",
            "     mu= 11.0: val_bal=0.4343 val_cov=0.800\n",
            "     mu= 15.0: val_bal=0.4280 val_cov=0.800\n",
            "     mu= 20.0: val_bal=0.4215 val_cov=0.800\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 2: mu=21.00 val_bal=0.4182\n",
            "     Refine 4: mu=20.75 val_bal=0.4180\n",
            "   Final selection: mu=20.75 val_bal=0.4180 val_cov=0.800\n",
            "   Best alpha: [1.63100265 0.04446097]\n",
            "   Alpha sum: 1.6755 (should be â‰ˆ 1.6000)\n",
            "   Alpha_hat (=alpha/K): [0.81550133 0.02223048]\n",
            "   Target=0.2  VAL: bal=0.4180 cov=0.800\n",
            "   Target=0.2  TEST: bal=0.4319 cov=0.800\n",
            "      VAL: worst=0.5016 | head=0.5016 | tail=0.3343 | tail-head=-0.1674\n",
            "      TEST: worst=0.5024 | head=0.5024 | tail=0.3615 | tail-head=-0.1409\n",
            "\n",
            "=== Target 4/9: rejection=0.3 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.60597624 0.03051833]\n",
            "     mu= -2.0: alpha=[1.59060253 0.03132218]\n",
            "     mu= -1.0: alpha=[1.58743424 0.03160215]\n",
            "     mu=  0.0: alpha=[1.58262601 0.03162223]\n",
            "     mu=  1.0: alpha=[1.57487496 0.03154849]\n",
            "     mu=  2.0: alpha=[1.56819416 0.03181668]\n",
            "     mu=  3.0: alpha=[1.56681386 0.03203123]\n",
            "     mu=  5.0: alpha=[1.55924002 0.03234235]\n",
            "     mu=  6.0: alpha=[1.54669988 0.03263618]\n",
            "     mu=  8.0: alpha=[1.54611358 0.03276148]\n",
            "     mu= 11.0: alpha=[1.52536878 0.03389904]\n",
            "     mu= 15.0: alpha=[1.50691013 0.03609255]\n",
            "     mu= 20.0: alpha=[1.46162603 0.03853483]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.4508 val_cov=0.700\n",
            "     mu= -2.0: val_bal=0.4503 val_cov=0.700\n",
            "     mu= -1.0: val_bal=0.4440 val_cov=0.700\n",
            "     mu=  0.0: val_bal=0.4414 val_cov=0.700\n",
            "     mu=  1.0: val_bal=0.4425 val_cov=0.700\n",
            "     mu=  2.0: val_bal=0.4355 val_cov=0.700\n",
            "     mu=  3.0: val_bal=0.4348 val_cov=0.700\n",
            "     mu=  5.0: val_bal=0.4288 val_cov=0.700\n",
            "     mu=  6.0: val_bal=0.4177 val_cov=0.700\n",
            "     mu=  8.0: val_bal=0.4149 val_cov=0.700\n",
            "     mu= 11.0: val_bal=0.4042 val_cov=0.700\n",
            "     mu= 15.0: val_bal=0.3967 val_cov=0.700\n",
            "     mu= 20.0: val_bal=0.3824 val_cov=0.700\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=22.00 val_bal=0.3769\n",
            "     Refine 2: mu=23.00 val_bal=0.3737\n",
            "   Final selection: mu=23.00 val_bal=0.3737 val_cov=0.700\n",
            "   Best alpha: [1.43280048 0.03923485]\n",
            "   Alpha sum: 1.4720 (should be â‰ˆ 1.4000)\n",
            "   Alpha_hat (=alpha/K): [0.71640024 0.01961742]\n",
            "   Target=0.3  VAL: bal=0.3737 cov=0.700\n",
            "   Target=0.3  TEST: bal=0.3870 cov=0.700\n",
            "      VAL: worst=0.4528 | head=0.4528 | tail=0.2946 | tail-head=-0.1582\n",
            "      TEST: worst=0.4515 | head=0.4515 | tail=0.3225 | tail-head=-0.1290\n",
            "\n",
            "=== Target 5/9: rejection=0.4 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.42935881 0.02349588]\n",
            "     mu= -2.0: alpha=[1.4228947  0.02474065]\n",
            "     mu= -1.0: alpha=[1.42038134 0.02536795]\n",
            "     mu=  0.0: alpha=[1.41978578 0.025577  ]\n",
            "     mu=  1.0: alpha=[1.41500749 0.02589049]\n",
            "     mu=  2.0: alpha=[1.40834329 0.02663054]\n",
            "     mu=  3.0: alpha=[1.4046212  0.02679948]\n",
            "     mu=  5.0: alpha=[1.39487824 0.02693316]\n",
            "     mu=  6.0: alpha=[1.37875025 0.02747472]\n",
            "     mu=  8.0: alpha=[1.38177353 0.02725931]\n",
            "     mu= 11.0: alpha=[1.37472243 0.02753043]\n",
            "     mu= 15.0: alpha=[1.33465276 0.02869807]\n",
            "     mu= 20.0: alpha=[1.28831717 0.03197421]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.4077 val_cov=0.600\n",
            "     mu= -2.0: val_bal=0.3932 val_cov=0.600\n",
            "     mu= -1.0: val_bal=0.3887 val_cov=0.600\n",
            "     mu=  0.0: val_bal=0.3903 val_cov=0.600\n",
            "     mu=  1.0: val_bal=0.3871 val_cov=0.600\n",
            "     mu=  2.0: val_bal=0.3697 val_cov=0.600\n",
            "     mu=  3.0: val_bal=0.3680 val_cov=0.600\n",
            "     mu=  5.0: val_bal=0.3648 val_cov=0.600\n",
            "     mu=  6.0: val_bal=0.3661 val_cov=0.600\n",
            "     mu=  8.0: val_bal=0.3610 val_cov=0.600\n",
            "     mu= 11.0: val_bal=0.3484 val_cov=0.600\n",
            "     mu= 15.0: val_bal=0.3287 val_cov=0.600\n",
            "     mu= 20.0: val_bal=0.3215 val_cov=0.600\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=18.00 val_bal=0.3168\n",
            "     Refine 4: mu=17.75 val_bal=0.3167\n",
            "   Final selection: mu=17.75 val_bal=0.3167 val_cov=0.600\n",
            "   Best alpha: [1.29624211 0.03043937]\n",
            "   Alpha sum: 1.3267 (should be â‰ˆ 1.2000)\n",
            "   Alpha_hat (=alpha/K): [0.64812106 0.01521969]\n",
            "   Target=0.4  VAL: bal=0.3167 cov=0.600\n",
            "   Target=0.4  TEST: bal=0.3475 cov=0.600\n",
            "      VAL: worst=0.4303 | head=0.4303 | tail=0.2030 | tail-head=-0.2273\n",
            "      TEST: worst=0.4130 | head=0.4130 | tail=0.2820 | tail-head=-0.1309\n",
            "\n",
            "=== Target 6/9: rejection=0.5 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.28188466 0.01723797]\n",
            "     mu= -2.0: alpha=[1.25560863 0.01835332]\n",
            "     mu= -1.0: alpha=[1.25668148 0.01824425]\n",
            "     mu=  0.0: alpha=[1.25275962 0.01837674]\n",
            "     mu=  1.0: alpha=[1.24018898 0.0192255 ]\n",
            "     mu=  2.0: alpha=[1.23822575 0.01958787]\n",
            "     mu=  3.0: alpha=[1.22994072 0.01999853]\n",
            "     mu=  5.0: alpha=[1.22248515 0.02011067]\n",
            "     mu=  6.0: alpha=[1.22196176 0.0205611 ]\n",
            "     mu=  8.0: alpha=[1.18734977 0.02133568]\n",
            "     mu= 11.0: alpha=[1.17269617 0.02230952]\n",
            "     mu= 15.0: alpha=[1.16133667 0.02412503]\n",
            "     mu= 20.0: alpha=[1.1352061  0.02575994]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.3460 val_cov=0.501\n",
            "     mu= -2.0: val_bal=0.3445 val_cov=0.501\n",
            "     mu= -1.0: val_bal=0.3312 val_cov=0.500\n",
            "     mu=  0.0: val_bal=0.3320 val_cov=0.501\n",
            "     mu=  1.0: val_bal=0.3209 val_cov=0.501\n",
            "     mu=  2.0: val_bal=0.3187 val_cov=0.501\n",
            "     mu=  3.0: val_bal=0.3215 val_cov=0.500\n",
            "     mu=  5.0: val_bal=0.3075 val_cov=0.501\n",
            "     mu=  6.0: val_bal=0.3014 val_cov=0.500\n",
            "     mu=  8.0: val_bal=0.3023 val_cov=0.501\n",
            "     mu= 11.0: val_bal=0.2937 val_cov=0.501\n",
            "     mu= 15.0: val_bal=0.2773 val_cov=0.501\n",
            "     mu= 20.0: val_bal=0.2640 val_cov=0.501\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=18.00 val_bal=0.2612\n",
            "     Refine 2: mu=19.00 val_bal=0.2609\n",
            "     Refine 3: mu=19.50 val_bal=0.2587\n",
            "   Final selection: mu=19.50 val_bal=0.2587 val_cov=0.501\n",
            "   Best alpha: [1.13968608 0.0257601 ]\n",
            "   Alpha sum: 1.1654 (should be â‰ˆ 1.0020)\n",
            "   Alpha_hat (=alpha/K): [0.56984304 0.01288005]\n",
            "   Target=0.5  VAL: bal=0.2587 cov=0.501\n",
            "   Target=0.5  TEST: bal=0.2913 cov=0.500\n",
            "      VAL: worst=0.3630 | head=0.3630 | tail=0.1543 | tail-head=-0.2087\n",
            "      TEST: worst=0.3489 | head=0.3489 | tail=0.2338 | tail-head=-0.1151\n",
            "\n",
            "=== Target 7/9: rejection=0.6 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[1.0735667  0.01129521]\n",
            "     mu= -2.0: alpha=[1.06367181 0.01205632]\n",
            "     mu= -1.0: alpha=[1.05805973 0.01224362]\n",
            "     mu=  0.0: alpha=[1.05179326 0.01349563]\n",
            "     mu=  1.0: alpha=[1.04786912 0.013412  ]\n",
            "     mu=  2.0: alpha=[1.05080868 0.01358441]\n",
            "     mu=  3.0: alpha=[1.04409106 0.01385464]\n",
            "     mu=  5.0: alpha=[1.04159227 0.01489027]\n",
            "     mu=  6.0: alpha=[1.03576753 0.01542757]\n",
            "     mu=  8.0: alpha=[1.02799667 0.01590159]\n",
            "     mu= 11.0: alpha=[1.01902167 0.01621863]\n",
            "     mu= 15.0: alpha=[1.00562781 0.01749834]\n",
            "     mu= 20.0: alpha=[0.9759198  0.01863086]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.2919 val_cov=0.401\n",
            "     mu= -2.0: val_bal=0.2668 val_cov=0.400\n",
            "     mu= -1.0: val_bal=0.2674 val_cov=0.401\n",
            "     mu=  0.0: val_bal=0.2552 val_cov=0.401\n",
            "     mu=  1.0: val_bal=0.2532 val_cov=0.401\n",
            "     mu=  2.0: val_bal=0.2530 val_cov=0.401\n",
            "     mu=  3.0: val_bal=0.2552 val_cov=0.401\n",
            "     mu=  5.0: val_bal=0.2404 val_cov=0.401\n",
            "     mu=  6.0: val_bal=0.2460 val_cov=0.401\n",
            "     mu=  8.0: val_bal=0.2447 val_cov=0.401\n",
            "     mu= 11.0: val_bal=0.2395 val_cov=0.400\n",
            "     mu= 15.0: val_bal=0.2367 val_cov=0.401\n",
            "     mu= 20.0: val_bal=0.2286 val_cov=0.401\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=18.00 val_bal=0.2262\n",
            "   Final selection: mu=18.00 val_bal=0.2262 val_cov=0.401\n",
            "   Best alpha: [0.97605132 0.01849877]\n",
            "   Alpha sum: 0.9946 (should be â‰ˆ 0.8020)\n",
            "   Alpha_hat (=alpha/K): [0.48802566 0.00924938]\n",
            "   Target=0.6  VAL: bal=0.2262 cov=0.401\n",
            "   Target=0.6  TEST: bal=0.2303 cov=0.400\n",
            "      VAL: worst=0.3250 | head=0.3250 | tail=0.1274 | tail-head=-0.1976\n",
            "      TEST: worst=0.2863 | head=0.2863 | tail=0.1743 | tail-head=-0.1120\n",
            "\n",
            "=== Target 8/9: rejection=0.7 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[0.89262986 0.00636331]\n",
            "     mu= -2.0: alpha=[0.88363411 0.00732038]\n",
            "     mu= -1.0: alpha=[0.88665961 0.00752995]\n",
            "     mu=  0.0: alpha=[0.88520836 0.00781951]\n",
            "     mu=  1.0: alpha=[0.87528397 0.00800742]\n",
            "     mu=  2.0: alpha=[0.87144759 0.00834406]\n",
            "     mu=  3.0: alpha=[0.86757538 0.0084711 ]\n",
            "     mu=  5.0: alpha=[0.86046516 0.00886515]\n",
            "     mu=  6.0: alpha=[0.85137282 0.01016632]\n",
            "     mu=  8.0: alpha=[0.84651254 0.0104404 ]\n",
            "     mu= 11.0: alpha=[0.82649738 0.01220132]\n",
            "     mu= 15.0: alpha=[0.82523125 0.01300106]\n",
            "     mu= 20.0: alpha=[0.78004522 0.0146032 ]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.2366 val_cov=0.301\n",
            "     mu= -2.0: val_bal=0.2166 val_cov=0.301\n",
            "     mu= -1.0: val_bal=0.2145 val_cov=0.301\n",
            "     mu=  0.0: val_bal=0.2268 val_cov=0.301\n",
            "     mu=  1.0: val_bal=0.2188 val_cov=0.301\n",
            "     mu=  2.0: val_bal=0.2117 val_cov=0.301\n",
            "     mu=  3.0: val_bal=0.2054 val_cov=0.301\n",
            "     mu=  5.0: val_bal=0.2119 val_cov=0.301\n",
            "     mu=  6.0: val_bal=0.2023 val_cov=0.300\n",
            "     mu=  8.0: val_bal=0.1958 val_cov=0.301\n",
            "     mu= 11.0: val_bal=0.1880 val_cov=0.301\n",
            "     mu= 15.0: val_bal=0.1811 val_cov=0.301\n",
            "     mu= 20.0: val_bal=0.1604 val_cov=0.301\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=22.00 val_bal=0.1576\n",
            "     Refine 2: mu=23.00 val_bal=0.1553\n",
            "   Final selection: mu=23.00 val_bal=0.1553 val_cov=0.301\n",
            "   Best alpha: [0.74583947 0.01696585]\n",
            "   Alpha sum: 0.7628 (should be â‰ˆ 0.6020)\n",
            "   Alpha_hat (=alpha/K): [0.37291973 0.00848293]\n",
            "   Target=0.7  VAL: bal=0.1553 cov=0.301\n",
            "   Target=0.7  TEST: bal=0.1589 cov=0.300\n",
            "      VAL: worst=0.2144 | head=0.2144 | tail=0.0962 | tail-head=-0.1183\n",
            "      TEST: worst=0.1815 | head=0.1815 | tail=0.1363 | tail-head=-0.0452\n",
            "\n",
            "=== Target 9/9: rejection=0.8 ===\n",
            "   Step 1: Optimizing (alpha, mu) on tunev for each mu...\n",
            "     mu= -5.0: alpha=[0.63933264 0.0028951 ]\n",
            "     mu= -2.0: alpha=[0.62819004 0.00370687]\n",
            "     mu= -1.0: alpha=[0.62822662 0.00407514]\n",
            "     mu=  0.0: alpha=[0.62517974 0.0041755 ]\n",
            "     mu=  1.0: alpha=[0.61595374 0.00444362]\n",
            "     mu=  2.0: alpha=[0.61564386 0.00475799]\n",
            "     mu=  3.0: alpha=[0.61983828 0.00496795]\n",
            "     mu=  5.0: alpha=[0.61065088 0.00516377]\n",
            "     mu=  6.0: alpha=[0.60675968 0.0055413 ]\n",
            "     mu=  8.0: alpha=[0.60309916 0.00566888]\n",
            "     mu= 11.0: alpha=[0.59778338 0.00645165]\n",
            "     mu= 15.0: alpha=[0.56652296 0.00913142]\n",
            "     mu= 20.0: alpha=[0.56148456 0.00995441]\n",
            "   Step 2: Selecting best mu based on val performance...\n",
            "     mu= -5.0: val_bal=0.1439 val_cov=0.201\n",
            "     mu= -2.0: val_bal=0.1302 val_cov=0.201\n",
            "     mu= -1.0: val_bal=0.1214 val_cov=0.200\n",
            "     mu=  0.0: val_bal=0.1196 val_cov=0.201\n",
            "     mu=  1.0: val_bal=0.1182 val_cov=0.201\n",
            "     mu=  2.0: val_bal=0.1102 val_cov=0.201\n",
            "     mu=  3.0: val_bal=0.1109 val_cov=0.201\n",
            "     mu=  5.0: val_bal=0.0945 val_cov=0.201\n",
            "     mu=  6.0: val_bal=0.0848 val_cov=0.201\n",
            "     mu=  8.0: val_bal=0.0862 val_cov=0.201\n",
            "     mu= 11.0: val_bal=0.1080 val_cov=0.201\n",
            "     mu= 15.0: val_bal=0.0842 val_cov=0.201\n",
            "     mu= 20.0: val_bal=0.0952 val_cov=0.201\n",
            "   Step 3: Local refinement around best mu...\n",
            "     Refine 1: mu=17.00 val_bal=0.0749\n",
            "   Final selection: mu=17.00 val_bal=0.0749 val_cov=0.201\n",
            "   Best alpha: [0.56334344 0.00928449]\n",
            "   Alpha sum: 0.5726 (should be â‰ˆ 0.4020)\n",
            "   Alpha_hat (=alpha/K): [0.28167172 0.00464224]\n",
            "   Target=0.8  VAL: bal=0.0749 cov=0.201\n",
            "   Target=0.8  TEST: bal=0.0792 cov=0.200\n",
            "      VAL: worst=0.0913 | head=0.0913 | tail=0.0586 | tail-head=-0.0327\n",
            "      TEST: worst=0.0991 | head=0.0991 | tail=0.0593 | tail-head=-0.0398\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1057: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(e_val, r_val))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1062: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(e_test, r_test))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1067: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(w_val, r_val))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1072: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(w_test, r_test))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1081: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(e_val[mask_val_08], r_val[mask_val_08]))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1086: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(w_val[mask_val_08], r_val[mask_val_08]))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1097: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(e_test[mask_test_08], r_test[mask_test_08]))\n",
            "/content/capstone_l2r_ess/run_balanced_plugin_gating.py:1104: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(w_test[mask_test_08], r_test[mask_test_08]))\n",
            "Saved results to: results/ltr_plugin/cifar100_lt_if100/ltr_plugin_gating_balanced.json\n",
            "Saved CSV results to: results/ltr_plugin/cifar100_lt_if100/ltr_plugin_gating_balanced.csv\n",
            "Val AURC - Balanced: 0.2492 | Worst-group: 0.3138\n",
            "Test AURC - Balanced: 0.2619 | Worst-group: 0.3057\n",
            "Val AURC (coverage>=0.2) - Balanced: 0.2492 | Worst-group: 0.3138\n",
            "Test AURC (coverage>=0.2) - Balanced: 0.2619 | Worst-group: 0.3057\n",
            "Saved combined plot to: results/ltr_plugin/cifar100_lt_if100/ltr_rc_curves_balanced_gating_test.png\n",
            "Saved gap plot to: results/ltr_plugin/cifar100_lt_if100/ltr_tail_minus_head_gating_test.png\n"
          ]
        }
      ],
      "source": [
        "!python run_balanced_plugin_gating.py --dataset cifar100_lt_if100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N2ayZe41YGJ",
        "outputId": "bbda916e-0210-430e-8802-d373c120b8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Using dataset: cifar100_lt_if100\n",
            "  Classes: 100\n",
            "  Experts: ['ce_baseline', 'logitadjust_baseline', 'balsoftmax_baseline']\n",
            "/content/capstone_l2r_ess/run_worst_plugin_gating.py:722: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(ew, r)) if r.size > 1 else float(ew.mean() if ew.size else 0.0)\n",
            "/content/capstone_l2r_ess/run_worst_plugin_gating.py:725: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  float(np.trapz(eb, r)) if r.size > 1 else float(eb.mean() if eb.size else 0.0)\n",
            "Saved results to: results/ltr_plugin/cifar100_lt_if100/ltr_plugin_gating_worst.json\n",
            "Saved plot to: results/ltr_plugin/cifar100_lt_if100/ltr_rc_curves_balanced_worst_gating_test.png\n",
            "Saved gap plot to: results/ltr_plugin/cifar100_lt_if100/ltr_tail_minus_head_worst_gating_test.png\n",
            "Saved CSV results to: results/ltr_plugin/cifar100_lt_if100/ltr_plugin_gating_worst.csv\n"
          ]
        }
      ],
      "source": [
        "!python run_worst_plugin_gating.py --dataset cifar100_lt_if100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKOLG26E2ei2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
